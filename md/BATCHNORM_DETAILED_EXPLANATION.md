# BatchNorm ä¸ batch_size=1 é—®é¢˜æ·±åº¦è§£æ

## ğŸ” é—®é¢˜ç°è±¡

**é”™è¯¯ä¿¡æ¯:**
```
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED.
This error may appear if you passed in a non-contiguous input.
```

**çœŸå®åŸå› :** ä¸æ˜¯è¾“å…¥ä¸è¿ç»­,è€Œæ˜¯ **BatchNorm é‡åˆ°äº† batch_size=1**!

---

## ğŸ—ï¸ BatchNorm å±‚åœ¨å“ªé‡Œ?

### 1. **ResNet18 çš„ç»“æ„**

åœ¨ä½ çš„ä»£ç ä¸­ (`train_gradcam.py:162`):

```python
resnet = models.resnet18(pretrained=True)
self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])
```

**ResNet18 çš„å®Œæ•´å±‚çº§ç»“æ„:**

```
ResNet18 = [
    0: Conv2d(3, 64, 7Ã—7, stride=2)          # åˆå§‹å·ç§¯
    1: BatchNorm2d(64)                        # â† BatchNorm å±‚ 1
    2: ReLU(inplace=True)
    3: MaxPool2d(3Ã—3, stride=2)

    4: layer1 (æ®‹å·®å—ç»„1)
       â”œâ”€ BasicBlock 1
       â”‚   â”œâ”€ Conv2d(64, 64, 3Ã—3)
       â”‚   â”œâ”€ BatchNorm2d(64)                 # â† BatchNorm å±‚
       â”‚   â”œâ”€ ReLU
       â”‚   â”œâ”€ Conv2d(64, 64, 3Ã—3)
       â”‚   â””â”€ BatchNorm2d(64)                 # â† BatchNorm å±‚
       â””â”€ BasicBlock 2
           â”œâ”€ Conv2d(64, 64, 3Ã—3)
           â”œâ”€ BatchNorm2d(64)                 # â† BatchNorm å±‚
           â”œâ”€ ReLU
           â”œâ”€ Conv2d(64, 64, 3Ã—3)
           â””â”€ BatchNorm2d(64)                 # â† BatchNorm å±‚

    5: layer2 (æ®‹å·®å—ç»„2) - åŒ…å«å¤šä¸ª BatchNorm
    6: layer3 (æ®‹å·®å—ç»„3) - åŒ…å«å¤šä¸ª BatchNorm
    7: layer4 (æ®‹å·®å—ç»„4) - åŒ…å«å¤šä¸ª BatchNorm

    8: AdaptiveAvgPool2d                      # è¢«ä½ å»æ‰äº†
    9: Linear(512, 1000)                      # è¢«ä½ å»æ‰äº†
]
```

**ä½ çš„ `feature_extractor` åŒ…å«:**
- åˆå§‹å·ç§¯å±‚ + **BatchNorm**
- layer1 (2ä¸ªæ®‹å·®å—, æ¯å—2ä¸ª **BatchNorm**) = 4ä¸ª BatchNorm
- layer2 (2ä¸ªæ®‹å·®å—) = 4ä¸ª BatchNorm
- layer3 (2ä¸ªæ®‹å·®å—) = 4ä¸ª BatchNorm
- layer4 (2ä¸ªæ®‹å·®å—) = 4ä¸ª BatchNorm

**æ€»å…±æœ‰ 17 ä¸ª BatchNorm å±‚!**

---

## ğŸ§  BatchNorm çš„å·¥ä½œåŸç†

### **è®­ç»ƒæ¨¡å¼ (train mode)**

BatchNorm çš„æ•°å­¦å…¬å¼:

```
1. è®¡ç®— batch çš„å‡å€¼å’Œæ–¹å·®:
   Î¼_batch = (1/N) Ã— Î£ x_i          # N æ˜¯ batch_size
   ÏƒÂ²_batch = (1/N) Ã— Î£ (x_i - Î¼)Â²

2. æ ‡å‡†åŒ–:
   xÌ‚_i = (x_i - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)

3. ç¼©æ”¾å’Œåç§»:
   y_i = Î³ Ã— xÌ‚_i + Î²
```

**å…³é”®ç‚¹:** éœ€è¦è®¡ç®— **batch çš„ç»Ÿè®¡é‡** (å‡å€¼å’Œæ–¹å·®)

### **å½“ batch_size=1 æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆ?**

```python
# å‡è®¾ batch_size=1
batch = [x_1]  # åªæœ‰ 1 ä¸ªæ ·æœ¬

# è®¡ç®—å‡å€¼
Î¼_batch = x_1  # åªæœ‰ 1 ä¸ªå€¼,å‡å€¼å°±æ˜¯å®ƒæœ¬èº«

# è®¡ç®—æ–¹å·®
ÏƒÂ²_batch = (1/1) Ã— (x_1 - Î¼_batch)Â²
         = (x_1 - x_1)Â²
         = 0                        # â† æ–¹å·®ä¸º 0!

# æ ‡å‡†åŒ–
xÌ‚_1 = (x_1 - Î¼_batch) / âˆš(0 + Îµ)
     = 0 / âˆšÎµ
     = 0                            # â† æ‰€æœ‰å€¼éƒ½å˜æˆ 0!
```

**é—®é¢˜:**
1. **æ–¹å·®ä¸º 0** â†’ æ ‡å‡†åŒ–åæ‰€æœ‰å€¼å˜ä¸º 0
2. **æ•°å€¼ä¸ç¨³å®š** â†’ æ¢¯åº¦è®¡ç®—å‡ºé—®é¢˜
3. **cuDNN ä¼˜åŒ–å¤±è´¥** â†’ cuDNN å†…éƒ¨ä¸æ”¯æŒè¿™ç§æƒ…å†µ

---

## ğŸ”¬ å®é™…é”™è¯¯å‘ç”Ÿçš„ä½ç½®

### **é”™è¯¯å †æ ˆåˆ†æ**

ä½ çš„é”™è¯¯å †æ ˆ:
```python
File "train_gradcam.py", line 238, in forward
    concat_feats = self.feature_extractor(concat_imgs)  # â† è¿™é‡Œ!

File ".../torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(                                 # â† BatchNorm å±‚

File ".../torch/nn/functional.py", line 2438, in batch_norm
    return torch.batch_norm(

RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED
```

**å…·ä½“ä½ç½®:**
1. ä½ è°ƒç”¨ `self.feature_extractor(concat_imgs)`
2. è¾“å…¥é€šè¿‡ç¬¬ä¸€ä¸ª Conv å±‚
3. è¿›å…¥ç¬¬ä¸€ä¸ª **BatchNorm2d(64) å±‚** (line 169)
4. BatchNorm å‘ç° batch_size=1
5. cuDNN æŠ¥é”™!

---

## ğŸ“Š ä¸ºä»€ä¹ˆä»¥å‰ä¸æŠ¥é”™?

### **å¯èƒ½çš„åŸå› :**

#### 1. **ä»¥å‰çš„ batch_size æ›´å¤§**
```python
# ä»¥å‰
batch_size = 8
æ•°æ®é›† = 10 ä¸ªæ ·æœ¬
æœ€åä¸€ä¸ª batch = [8, 2] â†’ 2 ä¸ªæ ·æœ¬,æ²¡é—®é¢˜ âœ…

# ç°åœ¨
batch_size = 8
æ•°æ®é›† = 9 ä¸ªæ ·æœ¬
æœ€åä¸€ä¸ª batch = [8, 1] â†’ 1 ä¸ªæ ·æœ¬,æŠ¥é”™! âŒ
```

#### 2. **ä»¥å‰çš„æ•°æ®åˆ’åˆ†ä¸åŒ**
```python
# ä»¥å‰: 8:2 åˆ’åˆ†
train = 8 ä¸ªæ ·æœ¬ â†’ batch: [8]
val   = 2 ä¸ªæ ·æœ¬ â†’ batch: [2] âœ…

# ç°åœ¨: 5:3:2 åˆ’åˆ†
train = 5 ä¸ªæ ·æœ¬ â†’ batch: [5]
val   = 3 ä¸ªæ ·æœ¬ â†’ batch: [3]
test  = 2 ä¸ªæ ·æœ¬ â†’ batch: [2]
# å¦‚æœæ€»å…±åªæœ‰ 6 ä¸ªæ ·æœ¬:
test  = 1 ä¸ªæ ·æœ¬ â†’ batch: [1] âŒ
```

#### 3. **ä»¥å‰æ²¡æœ‰æµ‹è¯•é›†**
```python
# ä»¥å‰: åªæœ‰ train å’Œ val
# ç°åœ¨: å¢åŠ äº† test é›†
# test é›†æ•°æ®å°‘,å®¹æ˜“å‡ºç° batch_size=1
```

---

## ğŸ’¡ BatchNorm çš„è®¾è®¡åˆè¡·

BatchNorm è®¾è®¡æ—¶å‡è®¾:
- **batch_size â‰¥ 2** (è‡³å°‘2ä¸ªæ ·æœ¬æ‰èƒ½è®¡ç®—æœ‰æ„ä¹‰çš„ç»Ÿè®¡é‡)
- æ¨è **batch_size â‰¥ 16** (ç»Ÿè®¡é‡æ›´ç¨³å®š)

**ä¸ºä»€ä¹ˆ batch_size=1 ä¸æ”¯æŒ?**
- å•ä¸ªæ ·æœ¬æ— æ³•è®¡ç®— batch ç»Ÿè®¡é‡
- æ–¹å·®ä¸º 0 å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
- PyTorch å’Œ cuDNN éƒ½ä¸æ”¯æŒè¿™ç§è¾¹ç•Œæƒ…å†µ

---

## ğŸ” å¦‚ä½•å®šä½é—®é¢˜å±‚?

### **æ–¹æ³• 1: æ‰“å°è°ƒè¯•**

```python
class ResNetAttentionFusion(nn.Module):
    def forward(self, x):
        if not isinstance(x, list):
            x = [x]

        concat_imgs = torch.cat(x, dim=0)
        print(f"concat_imgs shape: {concat_imgs.shape}")  # æ£€æŸ¥ batch_size

        if concat_imgs.size(0) == 1:
            print("âš ï¸  è­¦å‘Š: batch_size=1!")

        concat_feats = self.feature_extractor(concat_imgs)  # è¿™é‡ŒæŠ¥é”™
        # ...
```

### **æ–¹æ³• 2: é€å±‚æ£€æŸ¥**

```python
# æ‰‹åŠ¨è¿è¡Œæ¯ä¸€å±‚
for i, layer in enumerate(self.feature_extractor):
    print(f"Layer {i}: {layer}")
    try:
        concat_imgs = layer(concat_imgs)
        print(f"  è¾“å‡º shape: {concat_imgs.shape}")
    except RuntimeError as e:
        print(f"  âŒ æŠ¥é”™: {e}")
        break
```

### **æ–¹æ³• 3: æŸ¥çœ‹æ¨¡å‹ç»“æ„**

```python
# æ‰“å°æ‰€æœ‰ BatchNorm å±‚
for name, module in model.named_modules():
    if isinstance(module, nn.BatchNorm2d):
        print(f"BatchNorm å±‚: {name}")

# è¾“å‡º:
# BatchNorm å±‚: feature_extractor.1
# BatchNorm å±‚: feature_extractor.4.0.bn1
# BatchNorm å±‚: feature_extractor.4.0.bn2
# BatchNorm å±‚: feature_extractor.4.1.bn1
# BatchNorm å±‚: feature_extractor.4.1.bn2
# ... (å…± 17 ä¸ª)
```

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆå¯¹æ¯”

### **æ–¹æ¡ˆ 1: é¿å… batch_size=1 (æ¨è)**

```python
# âœ… è‡ªåŠ¨è°ƒæ•´ batch_size
effective_batch_size = max(2, args.batch_size)

# âœ… drop_last=True (è®­ç»ƒæ—¶)
train_loader = DataLoader(..., drop_last=True)

# âœ… è·³è¿‡ batch_size=1 (éªŒè¯æ—¶)
if labels.size(0) == 1:
    continue
```

**ä¼˜ç‚¹:**
- ä¸ä¿®æ”¹æ¨¡å‹
- æ€§èƒ½æœ€å¥½
- æœ€ç®€å•

**ç¼ºç‚¹:**
- å¯èƒ½ä¸¢å¤±å°‘é‡æ•°æ® (è®­ç»ƒæ—¶)
- éœ€è¦æ£€æŸ¥æ•°æ®é›†å¤§å°

### **æ–¹æ¡ˆ 2: ä½¿ç”¨ GroupNorm æ›¿ä»£ BatchNorm**

```python
# ä¿®æ”¹ ResNet å®šä¹‰
def replace_batchnorm_with_groupnorm(model, num_groups=32):
    for name, module in model.named_children():
        if isinstance(module, nn.BatchNorm2d):
            # æ›¿æ¢ä¸º GroupNorm
            num_channels = module.num_features
            new_module = nn.GroupNorm(num_groups, num_channels)
            setattr(model, name, new_module)
        else:
            # é€’å½’æ›¿æ¢å­æ¨¡å—
            replace_batchnorm_with_groupnorm(module, num_groups)
    return model

# ä½¿ç”¨
resnet = models.resnet18(pretrained=True)
resnet = replace_batchnorm_with_groupnorm(resnet)
```

**ä¼˜ç‚¹:**
- æ”¯æŒ batch_size=1
- GroupNorm ä¸ä¾èµ– batch ç»Ÿè®¡é‡

**ç¼ºç‚¹:**
- éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹
- é¢„è®­ç»ƒæƒé‡ä¸èƒ½ç›´æ¥ç”¨
- æ€§èƒ½å¯èƒ½ä¸‹é™

### **æ–¹æ¡ˆ 3: åˆ‡æ¢åˆ° eval æ¨¡å¼**

```python
# eval æ¨¡å¼ä¸‹ BatchNorm ä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡
model.eval()

# ä½†æ³¨æ„: å¦‚æœæ¨¡å‹åˆšåˆå§‹åŒ–,å…¨å±€ç»Ÿè®¡é‡ä¸å‡†ç¡®
# éœ€è¦å…ˆåœ¨è®­ç»ƒé›†ä¸Šè¿è¡Œä¸€æ¬¡
```

**ä¼˜ç‚¹:**
- ä¸æŠ¥é”™

**ç¼ºç‚¹:**
- è®­ç»ƒæ—¶ä¸èƒ½ç”¨
- å…¨å±€ç»Ÿè®¡é‡å¯èƒ½ä¸å‡†ç¡®

---

## ğŸ¯ ä½ çš„ä»£ç ä¸­çš„ BatchNorm ä½ç½®æ€»ç»“

```python
# train_gradcam.py:162
resnet = models.resnet18(pretrained=True)

# ResNet18 ç»“æ„:
ResNet(
  (conv1): Conv2d(3, 64, 7Ã—7)
  (bn1): BatchNorm2d(64)              # â† BatchNorm 1
  (relu): ReLU()
  (maxpool): MaxPool2d(3Ã—3)

  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, 3Ã—3)
      (bn1): BatchNorm2d(64)          # â† BatchNorm 2
      (conv2): Conv2d(64, 64, 3Ã—3)
      (bn2): BatchNorm2d(64)          # â† BatchNorm 3
    )
    (1): BasicBlock(...)              # â† åˆæœ‰ 2 ä¸ª BatchNorm
  )

  (layer2): Sequential(...)           # â† 4 ä¸ª BatchNorm
  (layer3): Sequential(...)           # â† 4 ä¸ª BatchNorm
  (layer4): Sequential(...)           # â† 4 ä¸ª BatchNorm

  (avgpool): AdaptiveAvgPool2d       # ä½ çš„ä»£ç å»æ‰äº†è¿™ä¸ª
  (fc): Linear(512, 1000)            # ä½ çš„ä»£ç å»æ‰äº†è¿™ä¸ª
)

# ä½ çš„ feature_extractor åŒ…å«:
# conv1 + bn1 + relu + maxpool + layer1 + layer2 + layer3 + layer4
# = 1 + 4 + 4 + 4 + 4 = 17 ä¸ª BatchNorm å±‚
```

**ç¬¬ä¸€ä¸ªæŠ¥é”™çš„æ˜¯:** `bn1` (BatchNorm2d(64)) - ç´§è·Ÿåœ¨ç¬¬ä¸€ä¸ªå·ç§¯å±‚åé¢

---

## ğŸ“ æ€»ç»“

### **ä¸ºä»€ä¹ˆ batch_size=1 æŠ¥é”™?**

1. **BatchNorm éœ€è¦è®¡ç®— batch ç»Ÿè®¡é‡** (å‡å€¼ã€æ–¹å·®)
2. **batch_size=1 æ—¶æ–¹å·®ä¸º 0** â†’ æ•°å€¼ä¸ç¨³å®š
3. **cuDNN ä¼˜åŒ–ä¸æ”¯æŒè¿™ç§è¾¹ç•Œæƒ…å†µ** â†’ æŠ¥é”™

### **é—®é¢˜å±‚åœ¨å“ª?**

- **æ‰€æœ‰ BatchNorm å±‚** (ResNet18 æœ‰ 17 ä¸ª)
- **ç¬¬ä¸€ä¸ªæŠ¥é”™:** `feature_extractor[1]` (bn1)
- **ä½ç½®:** ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¹‹å

### **ä¸ºä»€ä¹ˆä»¥å‰ä¸æŠ¥é”™?**

- ä»¥å‰çš„ batch_size æ›´åˆç†
- ä»¥å‰çš„æ•°æ®åˆ’åˆ†æ²¡æœ‰äº§ç”Ÿ batch_size=1
- å¢åŠ æµ‹è¯•é›†å,æ•°æ®æ›´åˆ†æ•£,æ›´å®¹æ˜“è§¦å‘

### **æœ€ä½³è§£å†³æ–¹æ¡ˆ:**

âœ… **é¿å… batch_size=1**
- è‡ªåŠ¨è°ƒæ•´ batch_size
- drop_last=True (è®­ç»ƒ)
- å¼‚å¸¸å¤„ç† (æµ‹è¯•)

âŒ **ä¸è¦:**
- ä¸è¦ä¿®æ”¹ BatchNorm (ä¼šå½±å“æ€§èƒ½)
- ä¸è¦è·³è¿‡æµ‹è¯•æ•°æ®
- ä¸è¦ä½¿ç”¨å¤ªå°çš„ batch_size

---

å¸Œæœ›è¿™ä¸ªè§£é‡Šæ¸…æ¥šäº†! æ ¸å¿ƒå°±æ˜¯: **ResNet18 é‡Œæœ‰ 17 ä¸ª BatchNorm å±‚,å®ƒä»¬éƒ½éœ€è¦ batch_size â‰¥ 2** ğŸ¯
