# NaN é—®é¢˜è§£å†³æŒ‡å—

## ğŸ”´ é—®é¢˜ç°è±¡

è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç° `loss=nan`ï¼ˆNot a Numberï¼‰ï¼Œè¡¨ç¤ºè®­ç»ƒæ•°å€¼ä¸ç¨³å®šã€‚

```
Epoch 1/50 | Train Loss: 0.4974 | Val Loss: 2.1927 | Val Acc: 0.0000
Epoch 2/50 | Train Loss: 0.7663 | Val Loss: nan | Val Acc: 1.0000
Epoch 3/50 | Train Loss: nan | Val Loss: nan | Val Acc: 1.0000
```

## ğŸ” NaN å‡ºç°çš„åŸå› 

### 1. **æ¢¯åº¦çˆ†ç‚¸** (æœ€å¸¸è§)
- å­¦ä¹ ç‡è¿‡é«˜å¯¼è‡´å‚æ•°æ›´æ–°å¹…åº¦è¿‡å¤§
- æ¢¯åº¦å€¼è¶…å‡ºæµ®ç‚¹æ•°è¡¨ç¤ºèŒƒå›´

### 2. **æ•°å€¼æº¢å‡º**
- æ··åˆç²¾åº¦è®­ç»ƒ (fp16/bf16) å®¹æ˜“å¯¼è‡´æ•°å€¼æº¢å‡º
- Attention æœºåˆ¶ä¸­çš„ softmax è®¡ç®—äº§ç”Ÿæç«¯å€¼

### 3. **BatchNorm é—®é¢˜**
- æ‰¹æ¬¡å¤ªå°ï¼ˆbatch_size=1ï¼‰å¯¼è‡´ç»Ÿè®¡é‡ä¸å‡†ç¡®
- å¤šGPUè®­ç»ƒæ—¶æ•°æ®åˆ†å¸ƒä¸å‡åŒ€

### 4. **æ•°æ®é—®é¢˜**
- è¾“å…¥æ•°æ®å­˜åœ¨å¼‚å¸¸å€¼ï¼ˆå¦‚æ— ç©·å¤§ã€NaNï¼‰
- æ ‡ç­¾é”™è¯¯

### 5. **æ¨¡å‹åˆå§‹åŒ–é—®é¢˜**
- æƒé‡åˆå§‹åŒ–ä¸å½“

## âœ… å·²å®æ–½çš„è§£å†³æ–¹æ¡ˆ

æˆ‘å·²ç»åœ¨ä»£ç ä¸­æ·»åŠ äº†ä»¥ä¸‹ä¿®å¤æªæ–½ï¼š

### 1. **æ¢¯åº¦è£å‰ª** (train.py:383-387)
```python
# æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
if accelerator:
    accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)
else:
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**ä½œç”¨**: é™åˆ¶æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°ä¸º 1.0ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 2. **Attention æ•°å€¼ç¨³å®šæ€§** (train.py:262-263)
```python
attn = self.attention(feats).squeeze(-1)
# æ·»åŠ æ•°å€¼ç¨³å®šæ€§ï¼šè£å‰ª attention åˆ†æ•°é˜²æ­¢æç«¯å€¼
attn = torch.clamp(attn, min=-10, max=10)
weights = torch.softmax(attn, dim=0).unsqueeze(-1)
```

**ä½œç”¨**: é™åˆ¶ attention åˆ†æ•°åœ¨ [-10, 10] èŒƒå›´å†…ï¼Œé˜²æ­¢ softmax äº§ç”Ÿæç«¯å€¼

### 3. **NaN æ£€æµ‹å’Œè·³è¿‡** (train.py:371-375)
```python
# æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸º NaN
if torch.isnan(loss) or torch.isinf(loss):
    if accelerator is None or accelerator.is_main_process:
        print(f"\nâš ï¸  è­¦å‘Š: æ£€æµ‹åˆ° NaN/Inf æŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
    continue
```

**ä½œç”¨**: å®æ—¶æ£€æµ‹ NaN/Infï¼Œè·³è¿‡æœ‰é—®é¢˜çš„æ‰¹æ¬¡ç»§ç»­è®­ç»ƒ

## ğŸš€ æ¨èä½¿ç”¨çš„è®­ç»ƒé…ç½®

### æ–¹æ³• 1: ä½¿ç”¨ç¨³å®šé…ç½®è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
./run_multi_gpu_stable.sh
```

è¿™ä¸ªè„šæœ¬åŒ…å«ä»¥ä¸‹ä¼˜åŒ–ï¼š
- âœ… é™ä½å­¦ä¹ ç‡: `5e-5` (åŸæ¥æ˜¯ `1e-4`)
- âœ… å…³é—­æ··åˆç²¾åº¦è®­ç»ƒ (ä½¿ç”¨ fp32)
- âœ… å¯ç”¨æ¢¯åº¦è£å‰ª
- âœ… å¯ç”¨ NaN æ£€æµ‹

### æ–¹æ³• 2: æ‰‹åŠ¨è®¾ç½®å‚æ•°

```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch \
    --config_file accelerate_config_stable.yaml \
    train.py \
    --batch-size 4 \
    --epochs 50 \
    --lr 5e-5 \
    --weight-decay 1e-5
```

## ğŸ“Š å‚æ•°è°ƒä¼˜å»ºè®®

### å­¦ä¹ ç‡è°ƒæ•´

æ ¹æ®è®­ç»ƒæƒ…å†µé€æ­¥è°ƒæ•´ï¼š

| ç°è±¡ | å­¦ä¹ ç‡è°ƒæ•´ | å»ºè®®å€¼ |
|------|-----------|--------|
| å¿«é€Ÿå‡ºç° NaN (1-3 epochs) | æ˜¾è‘—é™ä½ | `1e-5` æˆ– `5e-6` |
| ä¸­æœŸå‡ºç° NaN (10-20 epochs) | é€‚åº¦é™ä½ | `5e-5` |
| è®­ç»ƒç¨³å®šä½†æ”¶æ•›æ…¢ | é€‚åº¦æé«˜ | `1e-4` |
| ä¸€ç›´ç¨³å®š | å¯ä»¥å°è¯•æé«˜ | `2e-4` |

### æ‰¹æ¬¡å¤§å°è°ƒæ•´

```bash
# å¦‚æœæ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å¢å¤§ batch_size æé«˜ç¨³å®šæ€§
--batch-size 8   # æ¯ä¸ª GPU ä½¿ç”¨ 8
--batch-size 16  # æ¯ä¸ª GPU ä½¿ç”¨ 16
```

**æ³¨æ„**: batch_size è¶Šå¤§ï¼Œè®­ç»ƒè¶Šç¨³å®šï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜

### æ¢¯åº¦è£å‰ªå¼ºåº¦è°ƒæ•´

å¦‚æœä»ç„¶å‡ºç° NaNï¼Œå¯ä»¥é™ä½æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼ï¼š

ä¿®æ”¹ `train.py:385` ä¸­çš„ `max_norm`:
```python
# æ›´æ¿€è¿›çš„æ¢¯åº¦è£å‰ª
accelerator.clip_grad_norm_(model.parameters(), max_norm=0.5)  # ä» 1.0 é™åˆ° 0.5
```

## ğŸ”§ è¿›é˜¶è§£å†³æ–¹æ¡ˆ

### 1. ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼˜åŒ–å™¨

ä¿®æ”¹ `train.py:1119`:
```python
# ä½¿ç”¨ AdamW æ›¿ä»£ Adamï¼Œæ·»åŠ æƒé‡è¡°å‡
optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate,
                        weight_decay=1e-4, eps=1e-8)
```

### 2. ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­ (Warmup)

```python
from torch.optim.lr_scheduler import LinearLR, SequentialLR

# åˆ›å»ºé¢„çƒ­è°ƒåº¦å™¨
warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=5)
main_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs-5)

scheduler = SequentialLR(optimizer,
                        schedulers=[warmup_scheduler, main_scheduler],
                        milestones=[5])
```

### 3. æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰å¼‚å¸¸

æ·»åŠ æ•°æ®æ£€æŸ¥ä»£ç ï¼š
```python
# åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹å‰æ£€æŸ¥æ•°æ®
for seq_list, labels in train_loader:
    for seq in seq_list:
        if torch.isnan(seq).any() or torch.isinf(seq).any():
            print("âš ï¸  å‘ç°å¼‚å¸¸æ•°æ®!")
            break
```

### 4. ä½¿ç”¨æ›´ç¨³å®šçš„æŸå¤±å‡½æ•°

```python
# ä½¿ç”¨ label smoothing
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

## ğŸ“ è®­ç»ƒç›‘æ§å»ºè®®

### åœ¨è®­ç»ƒæ—¶æ·»åŠ é¢å¤–çš„ç›‘æ§

```bash
# åŒæ—¶è¿è¡Œ tensorboardï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
tensorboard --logdir=output/

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§ GPU
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ï¼š
```
output/train_YYYYMMDD_HHMMSS/logs/training_history.csv
```

å¯ä»¥ä½¿ç”¨ pandas åˆ†æï¼š
```python
import pandas as pd
df = pd.read_csv('output/train_YYYYMMDD_HHMMSS/logs/training_history.csv')
print(df[['epoch', 'train_loss', 'val_loss', 'val_acc']])
```

## ğŸ¯ å¿«é€Ÿæ£€æŸ¥æ¸…å•

é‡åˆ° NaN æ—¶æŒ‰ç…§ä»¥ä¸‹é¡ºåºæ£€æŸ¥ï¼š

- [ ] 1. ä½¿ç”¨ `run_multi_gpu_stable.sh` é‡æ–°è®­ç»ƒ
- [ ] 2. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡é«˜ï¼ˆé™ä½åˆ° `5e-5` æˆ–æ›´ä½ï¼‰
- [ ] 3. ç¡®è®¤å…³é—­äº†æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] 4. ç¡®è®¤å¯ç”¨äº†æ¢¯åº¦è£å‰ª
- [ ] 5. æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰å¼‚å¸¸å€¼
- [ ] 6. å¢å¤§ batch_sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
- [ ] 7. å°è¯•æ›´ä¿å®ˆçš„ä¼˜åŒ–å™¨è®¾ç½®

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå…³é—­æ··åˆç²¾åº¦è®­ç»ƒï¼Ÿ

**A**: æ··åˆç²¾åº¦ (fp16/bf16) è™½ç„¶èƒ½åŠ é€Ÿè®­ç»ƒï¼Œä½†æ•°å€¼è¡¨ç¤ºèŒƒå›´å°ï¼Œå®¹æ˜“æº¢å‡ºã€‚åœ¨è®­ç»ƒç¨³å®šåå¯ä»¥é‡æ–°å¯ç”¨ã€‚

### Q2: æ¢¯åº¦è£å‰ªä¼šå½±å“æ¨¡å‹æ€§èƒ½å—ï¼Ÿ

**A**: è½»å¾®çš„æ¢¯åº¦è£å‰ª (max_norm=1.0) é€šå¸¸ä¸ä¼šå½±å“æœ€ç»ˆæ€§èƒ½ï¼Œåè€Œèƒ½æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚

### Q3: é™ä½å­¦ä¹ ç‡ä¼šè®©è®­ç»ƒå˜æ…¢å—ï¼Ÿ

**A**: æ˜¯çš„ï¼Œä½†ç¨³å®šæ€§æ›´é‡è¦ã€‚å¯ä»¥é€šè¿‡å¢åŠ  epochs æ¥è¡¥å¿ã€‚

### Q4: NaN å‡ºç°åèƒ½æ¢å¤å—ï¼Ÿ

**A**: ä¸€æ—¦æ¨¡å‹å‚æ•°å˜æˆ NaNï¼Œé€šå¸¸æ— æ³•æ¢å¤ï¼Œéœ€è¦é‡æ–°å¼€å§‹è®­ç»ƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æ·»åŠ äº† NaN æ£€æµ‹å’Œè·³è¿‡æœºåˆ¶ã€‚

## ğŸ“š å‚è€ƒèµ„æ–™

- [PyTorch æ¢¯åº¦è£å‰ªæ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)
- [Accelerate æ··åˆç²¾åº¦è®­ç»ƒ](https://huggingface.co/docs/accelerate/usage_guides/mixed_precision)
- [è®­ç»ƒç¨³å®šæ€§æŠ€å·§](https://docs.fast.ai/callback.tracker.html)

---

å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. PyTorch ç‰ˆæœ¬æ˜¯å¦è¿‡æ—§
2. CUDA ç‰ˆæœ¬æ˜¯å¦å…¼å®¹
3. GPU é©±åŠ¨æ˜¯å¦æ­£å¸¸
4. æ•°æ®é›†æ˜¯å¦æŸå
